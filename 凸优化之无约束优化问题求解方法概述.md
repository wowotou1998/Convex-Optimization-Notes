## 无约束优化问题的求解方法

$$\text{minimize} \quad f(x)$$
其中 $f: \mathbf{R}^{n} \rightarrow \mathbf{R}$ 是二次可微凸函数(这意味着 $\operatorname{dom} f$ 是开集).我们假定该问题可解，即存在最优点 $x^{\star}$ . 更准确地说，$x^{\star}$ 不仅存在，并且唯一.我们用 $p^{\star}$ 表示最优值 $\underset{x}{\inf} f(x)=f\left(x^{\star}\right)$ . 既然 $f$ 是可微凸函数，最优点 $x^{\star}$ 应满足下述充要条件

$$
\nabla f\left(x^{\star}\right)=0
$$

因此，求解无约束优化问题等价于求解 $n$ 个变量 $x_{1}, \cdots, x_{n}$ 的$n$ 个方程.在一些特殊情况下，我们可以通过解析求解最优性方程 确定优化问题的解，但一般情况下，必须采用迭代算法求解方程，即计算点列 $x^{(0)}, x^{(1)}, \cdots \in \operatorname{dom} f$ 使得 $k \rightarrow \infty$ 时 $f\left(x^{(k)}\right) \rightarrow p^{\star}$ .这样的点列被称为优化问题的极小化点列.当 $f\left(x^{(k)}\right)-p^{\star} \leqslant \epsilon$ 时算法将终止，其中 $\epsilon>0$ 是设定的容许误差值.

## 初始点和下水平集

本章介绍的方法需要一个适当的初始点 $x^{(0)}$, 该初始点必须属于 $\operatorname{dom} f$ ，并且下 水平集
$$
S=\left\{x \in \operatorname{dom} f \mid f(x) \leqslant f\left(x^{(0)}\right)\right\}
$$
必须是闭集.如果 $f$ 是闭函数，即它的所有下水平集是闭集，上述条件对所有的 $x^{(0)} \in \operatorname{dom} f$ 均能满足.因为 $\operatorname{dom} f=\mathbf{R}^{n}$ 的连续函数是闭函数，所以如果 $\operatorname{dom} f=\mathbf{R}^{n}$, 任何 $x^{(0)}$ 均能满足初始下水平集条件.另一类重要的闭函数是其定义域为开集的连续函数，这类 $f(x)$ 将随着 $x$ 趋近 $\mathbf{b d} \operatorname{dom} f$ 而趋于无穷.

## 预备知识

### 偏导数, 梯度, 方向导数, 梯度方向始终是是函数的增大方向的解释

首先,偏导数是构成梯度和方向导数的基础,在求偏导数时,无论多么复杂的多元函数,在求偏导数时我们可以将原函数看成是一个简单的一元函数,然后将求偏导看成是对一元函数求导数,然而我们发现求出的导数会有一个很好的性质.
$$
\nabla f(x_{0}) =
\underset{\Delta x \rightarrow0}{\text{limit}}
\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}
$$
由于偏导数只考虑函数在某一个维度 $x^{i}$ 上的变化,因此函数在求导点 $x^{i}_{0}$ 的邻域内, 要么在这一维度的正方向上增大,要么在这一维度的负方向上增大.**而导数的正负恰恰向我们指明了函数增大的方向,或者说函数自己本身的趋势就向我们指明了导数的正负,函数在维度 $x^{i}$ 的 $x^{i}_{0}$ 点附近处于递增趋势时,求得的偏导数为正,则自变量向正方向移动会使函数值变大,反之,求得的偏导数为负,则自变量向负方向移动会使函数值变大.而梯度中的每一个元素的值都等于函数在某一维度上的偏导数的值, 因此梯度中的每一个元素都指向了函数在某一维上的增大方向,因此,梯度这个向量所代表的方向就表示了函数的增大方向.**

$$
\frac{\partial f}{\partial x^{i}_{0}} =
\underset{\Delta x^{i} \rightarrow 0}{\text{limit}}
\frac{f(x^{i}_{0}+\Delta x^{i})-f(x^{i}_{0})}{\Delta x^{i}}
$$

**梯度是一个向量, 而方向导数是一个标量.**  
若函数 $f(x)$ 在点 $x_{0}$ 处可微分，与方向 $l$ 同方向的单位向量为 $\boldsymbol{e}_{l}$, 则函数 $f(x)$ 在点 $x_{0}$ 处, 方向 $l$ 上的方向导数为:

$$
\begin{aligned}
\left.\frac{\partial f}{\partial l} \right |_{x_{0}}
=\nabla f(x_{0}) \cdot \boldsymbol{e}_{l}
\end{aligned}
$$

式中, $\theta$ 为梯度与方向 $l$ 的夹角  
(1) 当 $\theta=0$, 方向 $l$ 与梯度方向同向，函数 $f(x)$ 增长最快;  
(2) 当 $\theta=\pi$, 方向 $l$ 与梯度方向相反，函数 $f(x)$ 减少最快;  
(3) 当 $\theta=\pi / 2$, 方向 $l$ 与梯度方向正交，函数 $f(x)$ 变化率为0;  

### 梯度下降算法 Gradient Descent

用负梯度做搜索方向,令 $\nabla x = -\nabla f(x)$, 是一种自然的选择,相应的方法被称为梯度方法或者梯度下降算法.

梯度下降算法描述  
给定初始点 $x \in \operatorname{dom} f$.  
重复进行

1. $\Delta x:= -\nabla f(x)$.
2. **直线搜索**, 通过精确或回溯直线搜索方法确定步长 $t$
3. **修改**, $x:=x+ t \Delta x$ 直到满足停止准则.

停止准则通常取为 $\|\nabla f(x)\|_{2} \leqslant \eta$, 其中 $\eta$ 是小正数. 大部分情况下, 步骤 1 完成后就检验停止条件, 而不是在修改后才检验.  
收敛性:无论是精确搜索还是非精确搜索都是线性收敛

### 最速下降算法 Steepest Descent

对 $f(x+v)$ 在 $x$ 处进行一阶 Taylor 展开,
$$
f(x+v) \approx \widehat{f}(x+v)=f(x)+\nabla f(x)^{T} v
$$

其中右边第二项 **$\nabla f(x)^{T} v$ 是 $f$ 在 $x$ 处沿方向 $v$ 的方向导数**. 它近似给出了 $f$ 沿小的步径 $v$ 会发生的变化. 如果其方向导数是负数, 步径 $v$ 就是下降方向.  我们现在讨论如何选择 $v$ 使其方向导数尽可能小. 由于方向导数 $\nabla f(x)^{T} v$ 是 $v$ 的 线性函数，只要我们将 $v$ 取得充分大, 就可以使其方向导数充分小 (在 $v$ 是下降方向的前提下, 即 $\nabla f(x)^{T} v<0$ ). 为了使问题有意义，我们还必须限制 $v$ 的大小, 或者规范 $v$ 的长度. 不然只要 $v$ 足够大或者足够小, 方向导数就会很小.

首先, 假设我们选一个初始点 $x^{k}$, 令方向 $v \in R^{n}$, 那么 $\underset{x}{\min} f(x) \Leftrightarrow \underset{v}{\min} f\left(x^{k}+v\right)$.
这个时候我们把右边这个函数在 $x^{k}$ 展开, 则问题变成
$\underset{v}{\min} f\left(x^{k}\right)+\nabla f(x)^{T} v$
这个问题便是最速下降法本质上求解的问题.   
但是可以看到, 如果选的点 $\nabla f\left(x^{k}\right) \neq 0$ 的 话, 总是能够找到 $v$ 使得目标函数到负无穷. 所以我们需要对这个目标函数进行限制.  

#### (1) 用 $\|v\|_{1}=1(v中所有分量总和为1)$ 来限制  

$$d^{k+1}=\arg \underset{v}{\min}\left\{f\left(x^{k}\right)+\nabla f\left(x^{k}\right)^{T} v \mid\|v\|_{1}=1\right\}$$
此时是可以算出最优解 $v=d^{k+1}$ 的, 表达式为
$$
d^{k+1}=v=\left[\begin{array}{c}0 \\ \vdots \\ (-1) \operatorname{sgn}\left(\nabla f\left(x^{k}\right)_{i}\right) \\ \vdots \\ 0\end{array}\right]
$$
其中 $\nabla f\left(x^{k}\right)_{i}$ 满足 $\left|\nabla f\left(x^{k}\right)_{i}\right|=\max \left\{\left|\nabla f\left(x^{k}\right)_{1}\right|, \ldots,\left|\nabla f\left(x^{k}\right)_{n}\right|\right\}$
即，**最优方向与梯度的分量绝对值最大的方向相反**, 这也是为什么叫做最速 (最陡) 下降法的原因, 绝对值分量最大的那一个维度就是最陡的.  

#### (2) 用 $\|v\|_{2}=1$ 来限制

$$
d^{k+1}=\arg \underset{v}{\min}\left\{f\left(x^{k}\right)+\nabla f\left(x^{k}\right)^{T} v \mid\|v\|_{2}=1\right\}
$$
同理可得, $d^{k+1}=v=-\frac{\nabla f\left(x^{k}\right)}{\left\|\nabla f\left(x^{k}\right)\right\|_{2}}$
此时最速下降法就是梯度下降法给梯度做了个单位化, 与梯度下降法并无区别. 

#### (3) 用 $\|v\|_{\infty}=1(v中所有分量中最大值为1)$ 来限制

$$
d^{k+1}=\arg \underset{v}{\min}\left\{f\left(x^{k}\right)+\nabla f\left(x^{k}\right)^{T} v \mid\|v\|_{\infty}=1\right\}
$$

同理可得,
$$
\begin{aligned}
d^{k+1}
&=v
\\
&=[1, \cdots, 1] \cdot\left[\begin{array}{c}(-1) \operatorname{sgn}\left(\nabla f\left(x^{k}\right)_{1}\right) \\ \vdots \\ (-1) \operatorname{sgn}\left(\nabla f\left(x^{k}\right)_{n}\right)\end{array}\right]  
\end{aligned}
$$

最速下降方法使用最速下降方向作为直线搜索方向. 
收敛性:线性收敛.

### 坐标轮换法 Coordinate Descent

思路：
最速下降法实际上是找到梯度分量的绝对值最大的那一个维度(对方向 $v$ 的约束为1范数时)，然后沿着那个维度的反方向下降.  
但是这样找需要提前计算出梯度才行，反正都是沿着某一个维度/坐标方向下降，那么完全可以不计算梯度, 而是沿着每一个维度/坐标都下降一次，有点像穷举的办法. 
但是要注意的是, 说是说让迭代点沿着每一个坐标都下降一次. 不一定每一个坐标方向都真的会让函数值下降, 因此在每一次坐标下降后, 要对步长进行一次线搜索, 找到让函数值下降了的步长，然后继续坐标下降(更新 $\left.x_{i}^{k}\right)$, 搜索步长 $\alpha_{i}$, 依次下去把坐标都轮换一遍.  
具体算法过程: $x^{k}$, 单位向量 $e_{i}$, 第 $i$ 号元素为1,其他均为0
对第$i$个坐标轮换: 
$$
\begin{aligned}
&(d_{i}^{k})^{*}=\arg \underset{d_{i}^{k}}{\min}f(x^{k}+d_{i}^{k}e_{i}) \\
&使用精确搜索或者非精确搜索寻找步长t \\
&x_{i}^{k+1}:=x_{i}^{k} + t (d_{i}^{k})^{*} \\
\end{aligned}
$$

缺点: 对多维度的优化问题不友好, 计算量太大了. 
### 块坐标下降法 Block Coordinate Descent: 
要求 $f$ 为凸函数, 光滑,且只考虑变量 $x$ 和 $y$ 时,函数比较简单.
思路： 将自变量分成多个块, 比如 $f(x, y)$, 单纯的坐标轮换是对 $x_{1}, \ldots, x_{n}, y_{1}, \ldots, y_{m}$ 的 每一个维度去轮换，现在是固定一个块(比如固定 $y)$, 假定函数值的变化情况与另一块的自变量有关( $f(x)$ ), 然后去坐标轮换，轮换完了再固定这一块, 对之前那一块轮换.
具体算法过程:   
1. $f(x, y), x^{k}, y^{k}$固定 $y^{k}$ 不动, 输入 $x^{k}$, 对 $f\left(x, y^{k}\right)$ 执行与 $x$ 的坐标轮换, 得到 $x^{k+1}$
固定 $x^{k+1}$ 不动
2. 输入 $y^{k}$, 对 $f\left(x^{k+1}, y\right)$ 执行与 $y$ 的坐标轮换, 得到 $y^{k+1}$
3. 输入 $\left(x^{k+1}, y^{k+1}\right)$

### 牛顿法 Newton method
当目标函数二阶可微时，我们可以从最速下降法导出牛顿法.   
牛顿法的下降方向可以认为是在梯度下降法的基础上用Hessian矩阵对梯度做了一些修正
在目标二阶可微时, 把 $f\left(x^{k}+v\right)$ 二阶展开就可以导出牛顿法. 
$f\left(x^{k}+v\right)=f\left(x^{k}\right)+\nabla f\left(x^{k}\right)^{T} v+\frac{1}{2} v^{T} \nabla^{2} f\left(x^{k}\right)^{T} v$.
对 $v$ 求偏导得出 $d^{k}=v=-\left(\nabla^{2} f\left(x^{k}\right)\right)^{-1} \nabla\left(x^{k}\right)$
这个就是牛顿法的下降方向. 如果我们再把目光放到上述二阶展开式中可以发现, 如果迭代点的函数值 $f\left(x^{k}+v\right)$ 接近最优值 $f^{*}$ 时, 有函数的一阶导数 $\nabla f\left(x^{k}\right) \rightarrow 0$, 这意味展开式中 $f(x^{k}+v)与f(x^{k})相当接近$, 则 $\nabla f\left(x^{k}\right)^{T} v = f(x^{k}+v)-f(x^{k})$ 会变得很小, 将 $v$ 的最优值代入, 得到停止条件 
$$
\nabla f\left(x^{k}\right)^{T} v=\left\|\nabla f\left(x^{k}\right)^{T}\left(\nabla^{2} f\left(x^{k}\right)\right)^{-1} \nabla f\left(x^{k}\right)\right\| \leq \varepsilon
$$
$\varepsilon$ 足够小时(小于自己规定的精度), 算法差不多收敛了，可以不用继续迭代了. 
$$
\begin{array}{ll}
\exists \eta>0\\
\|\nabla f(x)\|_{2}<\eta \text{ 阻力收敛damped newton phase} \\
\|\nabla f(x)\|_{2}>\eta \text{ 二次收敛quadratically convergent} \\
最优解附近对函数进行一阶泰勒展开,一阶导数会接近于0,\\
因此,如果泰勒展开中一阶导数趋近于0,\\
那就说明我们快要接近最优值了(仅在凸问题中成立)
\end{array}
$$
  离最优解远时收敛的慢, 近时变得很快, 比线性收敛还快:

### 拟牛顿法 Quasi-Newton Method
拟牛顿法的迭代方向与牛顿法有略微不同,具体形式如下:
$$
d^{k}=-B \cdot \nabla f\left(x^{k}\right)
$$

这里里的 $B$ 是一个数值矩阵, 用来近似牛顿法中的 $\nabla^{2} f\left(x^{k}\right)$ 的.  $\nabla^{2} f\left(x^{k}\right)$ 的近似现在有许多的办法,, 不同的近似思路导致 $B$ 的计算模式也不同,产生了不同的拟牛顿算法,比如 $BFGS$ 法, $L-BFGS$ 法和 $DFP$ 法. 

## 参考资料
- [1] [Convex Optimization – Boyd and \nuandenberghe](https://web.stanford.edu/~boyd/c\nuxbook/)
- [2] [中科大-凸优化](https://www.bilibili.com/\nuideo/B\nu1Jt411p7jE)
- [2] [知乎-落日翻车-凸优化总结](https://zhuanlan.zhihu.com/p/198046559)